<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Multithreading and Concurrency in Java: Introduction to Concurrent Programming Transcript">
<title>Multithreading and Concurrency in Java: Introduction to Concurrent Programming Transcript</title>
</head>
<body>
<h1>Multithreading and Concurrency in Java: Introduction to Concurrent Programming</h1>
<p><p>Concurrent programming in Java allows you to run multiple sequences of operations at the same time using a variety of objects and mechanisms. In this course, you&#39;ll learn about concurrent programming concepts such as threads and processes, including working with multiple tasks, multithreading, and multiprocessing. You&#39;ll explore race conditions and concurrency challenges and how to achieve concurrent thread synchronization using locks. Next, you&#39;ll look at how semaphores can be used and conditions that may cause deadlocks. You&#39;ll examine Java objects that aid in working with multithreaded application, using atomic operations, and the benefits of using thread pools to manage multiple concurrently executing threads. Finally, you&#39;ll learn use cases of the Futures object and the Fork/Join framework in terms of executing multiple threads.</p></p>
<div>
<h2>Table of Contents</h2>
<ol><li><a href="#92582e31-0704-4590-a0ee-1e61ba83851b">Course Overview</a></li>
<li><a href="#acb0c72d-0654-43b9-bafc-bf429fac2ad9">Working with Multiple Tasks</a></li>
<li><a href="#6792eb2d-ec52-47e7-ab7a-efa06b35b541">Multithreading</a></li>
<li><a href="#9babd780-19ae-4b7b-b3a7-c2f71cae7dfa">Multiprocessing</a></li>
<li><a href="#7c3c6968-e554-4648-9ef4-c3b3836a56d2">Challenges with Concurrency</a></li>
<li><a href="#b50d41e4-0195-4068-a6f8-14b2cdf4a1bf">Synchronization and Locks</a></li>
<li><a href="#a15de653-e022-4808-ad0b-f41fb8d64ad0">Semaphores</a></li>
<li><a href="#8afdb5d9-a9b4-45df-b00c-820ed878608b">Deadlocks</a></li>
<li><a href="#90a5668f-09f5-43e7-91a0-ab9da1f2e628">Concurrency Objects in Java</a></li>
<li><a href="#fac1110c-2fd3-45cd-9a98-3c687a810206">Atomic Variables</a></li>
<li><a href="#96f23caf-e35e-4f84-a5c1-e98f97cd05a7">Executors and Thread Pools</a></li>
<li><a href="#e241a71a-8372-4fd8-b854-4e615c39cfd0">Future Objects and Fork/Join</a></li>
<li><a href="#98619706-a53d-4f55-9a5d-61fce5a67e18">Course Summary</a></li>
</ol></div>
<div>
<h2 id="92582e31-0704-4590-a0ee-1e61ba83851b">
Course Overview
</h2>
<div>[Video description begins] <em>Topic title: Course Overview</em> [Video description ends]
<p>Hi, and welcome to this course, An introduction to concurrent programming in Java. My name is Kishan Iyer and I will be your instructor for this course. <br /><br />[Video description begins] <em>Your host for this session is Kishan Iyer. He is a Software engineer and big data expert.</em> [Video description ends] <br /><br />A little about myself first. I have a master&#39;s degree in computer science from Columbia University, and have previously worked in companies such as Deutsche Bank and WebMD in New York. I presently work for LoonyCorn, a studio for high quality video content. Concurrent programming skills are among the most useful tools in the armory of any software engineer. The ability to run several tasks simultaneously even if they are sub-tasks created out of larger tasks can greatly improve your program&#39;s performance. This can save your organization a lot of time or even deliver a better experience to your end users.</p>
<p>And the Java programming language offers a variety of options when it comes to implementing concurrent programming through multi-threading. This course is purely theoretical. And the goal is to lay a firm foundation on which to build hands-on concurrent programming skills. We explore the concepts of multi-threading and multi-processing, the benefits of these approaches, as well as the potential pitfalls, such as inconsistencies caused by threads which are not synchronized.</p>
<p>We then delve into various synchronization options in the Java programming language, from synchronized functions and blocks, to the use of locks, which includes issues such as deadlocks, which may occur when using them. We then cover some of the more advanced topics related to multi-threading in Java, from executors which run tasks using a pool of threads, to the built in data structures, which make common operations thread safe. Once you have finished with this course, you will have a clear picture of what concurrent programming in Java involves, as well as the objects and frameworks which are available to implement multi-threading. You will be ready to move on to a more hands on course, covering the spawning and execution of concurrent threads in Java.</p></div>
</div>
<div>
<h2 id="acb0c72d-0654-43b9-bafc-bf429fac2ad9">
Working with Multiple Tasks
</h2>
<div>[Video description begins] <em>Topic title: Working with Multiple Tasks. Your host for this session is Kishan Iyer.</em> [Video description ends]
<p>In order to write code which implements concurrent programming, it is crucial to understand what exactly concurrent programming is and how multithreading fits into that picture. And with that in mind, we can now take a look at an example where we have multiple tasks to perform and different ways in which this can be carried out. And then we will tie this to concurrent programming. For our example, we start off with a rather mundane Saturday to-do list. It&#39;s a weekend and it&#39;s time for you to catch up on a lot of different chores in addition to what you may do on a regular weekday.</p>
<p>So let&#39;s just say, on your Saturday, you know that you need to prepare breakfast and lunch for you and your household, so you mark these as two separate tasks on your to-do list. Furthermore, let&#39;s just say you have a car which is in urgent need of repairs. For this, you&#39;ll need to drive the car over to the mechanic, wait while the mechanic examines and then maybe fixes the car, and then drive back home. The fact that you need to wait while the mechanic performs the fixes will come into play a little later.</p>
<p>So that makes up three different tasks on your to-do list but there&#39;s more, you have a vacation coming up for which you need to book both flights and hotels. So that makes two more tasks on your list, and then there is a lot of cleaning to be performed as well. In fact, you need to clean both the kitchen and the living room in your house. All in all, it&#39;s a total of seven different tasks which need to be performed. But we&#39;ll soon discuss the fact that not all of these tasks require your active involvement throughout the time when the task is being carried out. But for now, let&#39;s formulate a plan of action.</p>
<p>So let&#39;s just say this is what the plan looks like. Where you arrange the seven different tasks on your list in an order and then tackle them one by one. This is a straightforward sequential execution of the tasks, and also happens to be the simplest plan to come up with. You start off at hour zero by preparing breakfast for your household, and let&#39;s say this takes about one hour for you to carry out. So after that one hour, one of the tasks on your list has been checked off, and it&#39;s time for you to move on to the next task which is to get your car fixed.</p>
<p>Now let&#39;s just say, this involves a total of two hours to carry out. However, you need not be actively involved for the entire two hours. In fact, driving back and forth and also communicating with the mechanic is where your part comes into the picture, and this only takes about 30 minutes. And for the remaining hour and a half, well, you&#39;re just waiting for the mechanic to carry out the repairs. Now if you haven&#39;t quite planned anything while you wait for the mechanic, there is nothing you can do, in which case another two hours have elapsed. And at the end of three hours, you have carried out two out of the seven tasks on your list, and it&#39;s time for you to book the flights for your vacation.</p>
<p>Now this is something which takes about an hour, so after four hours, well one more task can be checked off. It&#39;s time then to prepare lunch for your household, but this is also something similar to getting the car fixed where only some of the portions require your active involvement. So let&#39;s just say the time taken for you to gather the ingredients, chop them up, and then put them into a cooker takes about 30 minutes. And then after that, you just need to shut the lid, turn on the cooker, and wait for another hour and a half for your food to be prepared.</p>
<p>So once again, let&#39;s assume that you haven&#39;t quite planned anything during the wait while you prepare your lunch. Which means that a further two hours have elapsed and at the end of six hours, you have carried out four out of your seven tasks. Then cleaning the kitchen requires one more hour and you go ahead and do that, and this is an actively involved task where you don&#39;t really wait for anything to take place. The same can also be said for booking hotels for your vacation. So once you turn your attention to that, after an hour, that task has been completed as well. And then the last thing for you to do is to clean up your living room, and after one more hour, you&#39;re finally done with all of your chores.</p>
<p>You can now kick back and relax a bit, and you may feel that this is especially deserved since you spent over nine hours doing this work. The question however is, did this really need to take all of nine hours? Well, the answer is no, and let&#39;s take a look at how exactly we could do better. We saw that there were a couple of tasks which did involve rather long waits, so in fact, we were sitting idle for much of the nine hours. This kind of wait becomes possible when we have someone or something else helping us carry out the task. It is the mechanic in the case of getting the car repaired, and in the case of preparing lunch, it is the cooker.</p>
<p>A more efficient use of the available time would be to switch over to some other pending task while we wait. Which means that while progress is being made on one task, we simply switch over to another and make sure that two tasks are making progress at the same time. To phrase it a little differently, we make sure that two jobs are being executed concurrently.</p>
<p>While this does sound great in terms of efficiency, it does require some management in order to carry out. For example, we will need to periodically switch between tasks, and we&#39;ll need some mechanism in place so that we can pick up where we left off when we switch back to a task which was only partially completed. So we now recognize that there are a lot of limitations to the sequential execution of tasks, especially when some of the tasks may involve a significant waiting period. In the next video, we will take a look at the use of multithreading in order to execute the same tasks more efficiently.</p></div>
</div>
<div>
<h2 id="6792eb2d-ec52-47e7-ab7a-efa06b35b541">
Multithreading
</h2>
<div>[Video description begins] <em>Topic title: Multithreading. Your host for this session is Kishan Iyer.</em> [Video description ends]
<p>In the previous video, we saw how a simple sequential execution of tasks can help us handle a lot of chores to be performed on a Saturday. We will now adopt the approach of multithreading to see that this can be done in a more efficient manner. So when we perform this sequential execution, a total of seven tasks were carried out in nine hours. A benefit of this approach is that we only need to focus on a single task at a time.</p>
<p>However, a number of these tasks involve long waits where we were waiting on, say, a mechanic to carry out the repairs on a car, or for a cooker to finish cooking a lunch. It&#39;s time now for us to modify this plan just a little bit and see what happens if we assume that we can manage the overhead of switching between tasks if that&#39;s what needs to be done. So once again, we start off by preparing breakfast. This is something which takes one hour and does not require any waits. So, this task has completed after one hour. So this is the same as the sequential plan so far. And then comes a task which does involve a wait. Getting the car fixed involves 30 minutes of involvement from us directly, but 90 minutes of waiting for a mechanic. But let&#39;s just say we decide to proceed with the next task on our list, while we have handed things over to the mechanic.</p>
<p>So after one hour or so, we have handed things over to the mechanic. And at this point, we decide to leave this partially completed task, and then switch over to the booking of flights. You will note that while this is going on, the car is getting fixed. And at the same time, we are booking flights. In other words, two tasks are taking place concurrently. And this is where we can introduce some efficiency in our plan. So let&#39;s just say after three hours, well, three of the tasks on our list have been completed. <br /><br />And we have already saved one hour of the sequential plan. And this is where we move ahead to preparing lunch. And once again, this is something which requires 30 minutes of involvement from us, and then a 90 minute wait. So let&#39;s just say another half an hour has passed, while we have put in all the ingredients into the cooker. And now it&#39;s just time for us to wait for 90 minutes for that task to complete. But, since we don&#39;t need to be actively involved at this point, well, we decide to switch over to the next task on our list, which is to clean the kitchen.</p>
<p>So this is something which requires one more hour of work. But of course, cleaning a kitchen can involve a lot of physical effort. So after about 30 minutes, well, at this point, we decide to switch tasks once again. This means that the cleaning kitchen task is only partially complete, as is the preparation of lunch. And we move ahead to the booking of hotels for our upcoming vacation. This means that we have three concurrent tasks at this point. The lunch preparation is going on. The cleaning of the kitchen is also in an ongoing state. And now the booking of hotels has also begun. Now let&#39;s just say a further one hour has passed, and the booking of hotels is now complete. <br /><br />And beyond that, the lunch preparation is also done. So five of the seven tasks on our list have now been knocked off. So while two tasks still need to be completed, one of them is also partially done. So when we switch back to the cleaning of the kitchen, we realize that all of the pending tasks now don&#39;t really require any waiting. Which means that we still have another 90 minutes of work. So let&#39;s just say, we do this in a sequential manner. <br /><br />So once a further 90 minutes have elapsed, we have finally completed all of the seven tasks on our to do list. And significantly, this particular plan of action saved us a total of two and a half hours compared to the sequential plan. And what we just carried out here is the equivalent of multithreading in a programming language. Where we switch between tasks which involve waiting periods, and this allows us to make progress on multiple tasks at the same time.</p>
<p>And all of this was made possible with a little additional overhead. We can now take a closer look at some of the properties of multithreading. This is where we have multiple jobs which are executed concurrently by the same process. In the world of programming, the term process does have a specific meaning, but in the case of our real world example of a Saturday to-do list, let&#39;s just say that the process is you. And we saw how you could potentially handle seven different tasks, and some of them could make progress at the same time. Multithreading does involve several threads sharing a context, but they should be able to make some progress independently of the others. In the case of programming, the shared context could refer to commonly used data structures as well as space within main memory.</p>
<p>And in the case of our real world example, the shared context could very well be your own head, where you keep track of the different tasks. One significant feature of multithreaded execution is that even though multiple tasks can be taking place concurrently, your program is only focused on one of them at any given time. On our Saturday to-do list, for example, while you&#39;re booking flights, you&#39;re only focused on that. Even while other tasks, such as the preparation of your lunch, is taking place separately. You can almost think of the lunch preparation being performed by the cooker being a different program which is helping you with your task. Which allows multiple jobs to proceed concurrently, even though you may only be focused on one of them at a time. <br /><br />Furthermore, when we have several concurrent threads executing, well, this means that they all happen to be in a partially completed state. And this is where management of the concurrent threads becomes a factor. So these are some of the main properties of a multithreading approach. When you have other programs to help you out rather than wait for them to get back to you, you can make progress on other tasks on your list. So what are the benefits of this approach? Well, we have covered the fact that it is possible for us to make at least incremental progress on all of the concurrent tasks. This is especially useful when we have many different things going on in an application.</p>
<p>So let&#39;s just say, your application has several concurrent users who happen to be downloading something rather than processing each of the downloads sequentially. If you adopt a multithreaded approach, well, all of your users will not feel that they&#39;re being ignored by your application. And in the case of most programming languages, switching between different threads is made rather simple and is highly optimized. So this does not really involve significant work in terms of development or significant resources in terms of task switching. And crucially, if many of the tasks involve long waits and idle time, well a multithreading approach does help us utilize this in an efficient manner. So what are some of the applications of multithreading?</p>
<p>Well, I/O bound tasks are a common use case. For example, let&#39;s just say the loading of a webpage involves the retrieval of text from one database, images from another, and videos from a third. We can have separate threads carry these out independently. So let&#39;s just say we have a single process which creates these different threads. It sends off one request in order to retrieve the text. And while that is being retrieved, it puts in another request to get the images. And then a third to retrieve the videos. And furthermore, the rendering of each of these can also be handled by different threads. <br /><br />Similarly, network bound tasks are also great candidates for a multithreading approach. Just like with I/O bound tasks, these may involve significant waits between the submission of a request for some data and receiving a response which contains that data. So we have now seen how a multithreaded approach towards tackling multiple tasks can help us save a lot of time compared to a sequential execution. But the question now is, can we do even better than this approach? And the answer is yes. And this is what we will explore in the next video when we tackle multiprocessing.</p></div>
</div>
<div>
<h2 id="9babd780-19ae-4b7b-b3a7-c2f71cae7dfa">
Multiprocessing
</h2>
<div>[Video description begins] <em>Topic title: Multiprocessing. Your host for this session is Kishan Iyer.</em> [Video description ends]
<p>In the previous video we discussed how in a multi threading approach a single program which runs on a single process is only able to process one thread at any given time. This is a limitation which could potentially be addressed with a multiprocessing approach. And it&#39;s time now for us to turn our attention towards this topic. We started off in this course by taking care of these seven tasks using a sequential plan of execution. While this did help us get the job done eventually, it did take us a total of nine hours. And then of course, we turned our attention towards a multi threaded plan which allowed us to make use of waiting periods in order to tackle multiple tasks at the same time.</p>
<p>And we saw how this could potentially help us shave off two and a half hours in our total execution time. So if you think of yourself as a process, even with a single process, adopting this multi threaded approach does have its benefits but can we do even better if we have multiple processes? So imagine that is not just you who needs to handle these tasks, but you do have someone else who is around to help you out. Intuitively, you may know that this can help speed things up. But let&#39;s now visualize how this works, so that we can better understand a multi processing approach when it comes to programming. So the green arrow on the left is you while the pinkish arrow on the right is your partner. Given the first two tasks on this list are completely independent of each other.</p>
<p>Well, let&#39;s just say both of you can make progress and after one hour, well, the making breakfast task is now completely checked off and getting the car fixed is also partially complete. So you are now free to move on to the next task on the list which is to book flights while your partner still focuses on getting the car repaired. Let&#39;s just assume for the sake of simplicity, that there is no multi threading approach adopted by either of you. So another hour has elapsed and at the end of hour two, well, three different tasks have been completed.</p>
<p>And it&#39;s time for each of you to switch over to one more of the pending tasks. Do keep in mind that in spite of the great progress being made, there is some amount of coordination which is required between you and your partner, so that you don&#39;t step on each other&#39;s toes. And we also assume that each of the tasks here are completely independent of the other, but this is what does allow concurrent executions to take place. So after hour three, the kitchen cleaning has been completed and you are halfway through the lunch preparation.<br /><br />Another hour has elapsed and you have now prepared lunch and your partner has finished the hotel booking for your vacation. So now just a single job remained from your to do list. And again for the sake of simplicity, we assume that this is a one person job to clean the living room, which means that you can now call it a day and your partner spends one more hour to perform this last task.</p>
<p>So at the end of five hours, all of your chores have now been tackled, which means that both of you are now free to relax and perhaps head out for a stroll in the park. Significantly, this plan has saved you a lot of time compared to the sequential execution and also the multi threading approach. And this is what is the equivalent of multi processing in programming. So what exactly are some of the properties of this approach? Well, much like multi threading, multi programming or multi processing also allows concurrent execution of different tasks. But they do occur on different processes rather than in different threads. And there is a subtle but significant difference between the two. And this is because concurrent processes run independently on different CPU cores.</p>
<p>And this is what allows for true parallel executions. This usually means that each independent process has its own CPU cache, as well as its own space in memory. So the sharing of resources is not as tightly bound as in a multi threaded approach. To get the best benefits of both multi processing and multi threading, we can in fact combine the two approaches to arrive at an optimal solution. In a Saturday to do list example, we could get the total time taken to less than five hours if we did adopt this combination. In fact, this is a crucial factor to consider when adopting multiprocessing. This only works if the processes themselves are constantly occupied and are not waiting on another operation.</p>
<p>Multi processing does involve a lot more overhead and multi threading. So if a lot of the inefficiencies in a sequential execution of your task is due to long waiting periods, then multi threading may in fact be better than multi processing in such cases. So how exactly does multi processing contrast with multi threading? Well, for one, they don&#39;t quite share data in the same way as threads. Different threads often share the same resources in terms of memory space, as well as CPU cache, but this does not apply with different processes. For the Saturday to do list, we did discuss the fact that some coordination between the two people involved will be required in order to avoid stepping on each other&#39;s toes, and this certainly applies to multi processing.</p>
<p>In cases where there are a large number of processes, you may in fact require a separate manager in order to coordinate their actions. All of this clearly involves overhead and this is something to consider when adopting a multi-processing approach. To compare with an example in the real world consider an organization which has some teams with a large number of workers and managers and others which have just a few of each. Some tasks are just better off being handled by smaller teams, and you must adopt the same thinking when it comes to building your programs. And of course, the root cause of all of this is that switching between different processes and coordinating their actions does involve plenty of overhead.</p>
<p>So given all of this, what exactly are some of the applications of multiprocessing? Well for example, if you have a number of CPU bound tasks, which can be executed somewhat independently. If you have complex numerical operations to be carried out, well, you can have different processes work on these separately. And the same also applies to image processing, where different processes could focus on different segments of the image. <br /><br />So now that we know what multithreading and multiprocessing are, how exactly does this tie in to the Java programming language? Well, most implementations of the Java Virtual Machine run as a single process. Which means that the programs which you build are usually meant to run on a single processor which means that multiprocessing in a single Java program is rare. What is a lot more common however is multithreading, and a single app which is running in a JVM is capable of spawning and managing multiple threads.</p>
<p>In fact this is the focus of our learning path. Furthermore threads in Java are very light weight and are in fact referred to as lightweight processes. This means that spawning executing and also switching between threads does not involve too much overhead and has been highly optimized by Java over the years. However, this does not mean that multi processing is not possible in Java at all. In fact, it is possible for us to use the process builder class in order to spawn and execute different OS level processes. And here&#39;s the quick word on the implementation of multi threading in a Java app.</p>
<p>And broadly speaking, there are two different ways in which we can define a task which is meant to run on a separate thread. One of these is to make use of the runnable interface so we can define a class which implements this, which allows that class to inherit from other classes. The other option is to extend the thread class rather than implement the runnable interface. But since the Java language does not permit multiple inheritance this prevents us from inheriting from other Java classes. This is why in most cases we are better off implementing the runnable interface rather than extending the thread class when it comes to defining our task which need to be executed.</p></div>
</div>
<div>
<h2 id="7c3c6968-e554-4648-9ef4-c3b3836a56d2">
Challenges with Concurrency
</h2>
<div>[Video description begins] <em>Topic title: Challenges with Concurrency. Your host for this session is Kishan Iyer.</em> [Video description ends]
<p>So far in this course, we have explored the concepts of multiprocessing as well as multithreading, when it comes to concurrent executions of tasks. What we have discussed so far has covered the benefits of those approaches. And the cost we have talked about, has only been the overhead of managing different processes and threads. Moreover, to keep things simple, we have assumed that each of the different tasks which can run in different threads or processes, are entirely independent of one another.</p>
<p>But of course, this is not really the case in real life. And we will now take a closer look at some of the challenges when it comes to concurrent programming. So we already know that concurrency can deliver significant benefits in terms of overall execution time, but at the cost of some overhead at the very least. But what are the things which could go wrong? That is, where the output of our program is different from what we expect, and what would have taken place had we performed a sequential execution of different tasks. Well, one of these is the race condition, which we will now examine with an example. Specifically, let&#39;s consider a series of banking transactions. For that, we assume that you have an account which has a total of $100 as balance, and one of your friends owes you $50 and initiates this transfer using some banking application.</p>
<p>Let&#39;s now take a look at the type of code which could be written for this. So you have three lines here, where you have one function in order to retrieve the current balance in your account. This is called get balance and once that is retrieved, you add the deposit amount to that balance and then invoke a set balance function, in order to update the final balance, so that you&#39;re left with a total of $150. Moving along then, let&#39;s just say, one more friend has initiated a similar transaction where she owes you $50 once again, and uses the same banking app to make this transfer, which of course contains the same lines of code. Once this is executed, you&#39;re left with a total of $200 in your bank account, and this is all well and good.</p>
<p>You started with $100, two friends gave you $50 each, and you end up with 200. So let&#39;s take a look at a slightly different scenario where your friends initiate their transfers not one after the other, but at about the same time. So you start off with $100 again, and both of your friends pull up their banking apps and then initiate their transfers at the same time, but the server which manages these banking transactions, decides to make things more efficient by running them as separate threads. But given the server app is just a single process, it can only execute one thread at a time.</p>
<p>However, that concurrent execution in separate threads can be managed by periodically switching between the threads. So how exactly could this occur? Well, in one scenario, let&#39;s assume that the first line of the first transfer gets executed. So this is where a call to get balance is made, and in this user&#39;s app, the balance has been retrieved as $100. Note that the code which is inside a yellow box represents lines which have not yet been executed. So at this point, let&#39;s just say a thread switch has occurred, and then the server program has switched over to the second transaction.</p>
<p>And this is where the value of balance have been retrieved as $100 once again. One more thread which has taken place where the first user&#39;s balance has been incremented by the deposit amount of 50. So the value of the balance variable is 150 in this case, and when another thread switch has occurred, this thread has also updated its balance variable to 150. And you may already notice the seeds of a potential problem. This can be confirmed if we go ahead with one more thread switch. And this is where the first thread calls the set balance function, and sets the final balance in her account as $150. And then with the final thread switch, the second thread does exactly the same.</p>
<p>So these are the exact same transactions using the same code which we saw previously, except when we adopt a multithreading approach rather than a sequential one, by virtue of bad timing, we could end up with an incorrect balance of $150. And this is what is known as a race condition. A rather nice definition of the race condition which I obtained from Wikipedia, is that where the behavior of your system very much depends on the sequence or even the timing of certain operations which are beyond your control. In the case of our example, the final balance in your account was influenced by the fact that two deposits to it were initiated at about the same time. So it&#39;s not just the nature of the transactions, but the timings which have determined how much money you end up with.</p>
<p>This is clearly not a desirable state of events, especially if you put yourself in the shoes of the developer of the banking application. So you would like your end users to view the correct results, no matter what the sequence of transactions. However, you would still like to read some of the benefits of a multithreading approach. Is it possible to have both of these? Well, yes, however, there is some work involved. And this is where we can synchronize the actions of concurrent threads and processes. In the next video, we will take a look at exactly what this may involve, and also some of the options available for synchronization of threads in the Java language.</p></div>
</div>
<div>
<h2 id="b50d41e4-0195-4068-a6f8-14b2cdf4a1bf">
Synchronization and Locks
</h2>
<div>[Video description begins] <em>Topic title: Synchronization and Locks. Your host for this session is Kishan Iyer.</em> [Video description ends]
<p>In the previous video, we saw precisely what a race condition is and how this can occur when we have several threads which are working on shared data. We also discussed the fact that a race condition can be avoided by implementing synchronization mechanisms between concurrent threads. We now take a closer look at what exactly is meant by synchronization and how this can be implemented in a language like Java. So in the example we&#39;ve been working with, we start off with a balance of $100 in the account and looked at a case where we have two deposits which are initiated at about the same time and are $50 each. We expect the final balance to be $200 in this case however, to make this happen while we use a multi-threading approach, we need to implement some synchronization.</p>
<p>This is something which will allow threads to execute concurrently, but at the point where they make use of a shared resource, such as the bank balance in this case, we can write our code in such a way where some synchronization is necessary. So let&#39;s just say in order to access and update this balance, our processes will need to acquire a lock and there&#39;s just a single lock which makes sure that only one thread can access this balance at any given time. So how exactly will this work? Well, we have three lines of code in each of our threads where a shared resource is being accessed and updated and we can write our code in such a way where to proceed with these lines of code. One of the threads needs to acquire a lock.</p>
<p>A feature of a lock is that only a single thread can acquire it at any given time but once it does have it, well, it can effectively unlock the resource and then proceed with its own execution. So thread A gets to execute line number one. And even if thread B request a lock at this time, it will not get access to it. In fact, it must wait for thread A to release it voluntarily. So what we have introduced with this locking mechanism is that these three lines of code cannot proceed concurrently. It will not get access to it. In fact, it must wait for thread A to release it voluntarily. So what we have introduced with this locking mechanism is that these three lines of code cannot proceed concurrently.</p>
<p>So since thread B must wait well thread A can go on with its remaining two lines and once it is done, well, it is done accessing the critical resource, and it can release the lock which it had acquired. At this point, any of the waiting threads can acquire the locks on their own and this is precisely what happens to thread B. Once again, as long as thread B holds the lock, no other thread can acquire it and update the balance. And once thread B is finished with its own execution, the final balance has been updated to 200 and this is precisely what we wanted in the first place. So what role do locks play in concurrent executions. Well, they help protect critical resources. And they make this happen by making sure that only a single thread or process can hold a lock at any given time. All of this has the effect of ensuring the overall consistency of the shared data.</p>
<p>And significantly, this allows us to execute concurrently except when it comes to the updating of critical resources. So this is where we introduce a blend of concurrent executions and sequential ones, where updates to critical resources need to happen sequentially, but everything else in a task can happen on a concurrent basis. All right, so now that we understand some of the concepts around locking, how exactly can this be achieved in Java? Well, synchronization in this language can be achieved by means of the synchronized keyword in order to grant exclusive access to a single thread to a specific section of code. For example, it is possible for us to declare a function to be synchronized. This means that only a single thread will execute that particular function at any given time and it is not possible for interleaving of the code to occur. So, if a function involves the update to a critical resource, it does make sense to declare it as a synchronized function.</p>
<p>On the other hand, this may be a rather blunt instrument, especially where a function is rather large and involves many lines of code, but only some of them involve updates to critical resources. In such cases, it makes sense to make use of a synchronized block, which is a feature in Java and allows specific lines of code to be synchronized even if they&#39;re just a few lines within a large function. In each of these cases, whether we apply synchronized functions or blocks to involve locking off the specific section of code and these are referred to as implicit locks in Java, this is to distinguish them from explicit locks which we now turn our attention to and in the Java programming language, this term pertains to specific locking interfaces and lock implementations which are defined in the java.util.concurrent.locks package. These are included because while synchronized functions and blocks do enable locking, their features don&#39;t extend beyond the acquisition and release of locks. When it comes to explicit locking, there are a number of other features which we can make use of.</p>
<p>For example, there is a condition interface which allows threads to synchronize their actions on the same object, but on different conditions. This also includes methods which thread can use to communicate with one another and these include the wait notify and signal methods among many others and we will explore these in the labs of this learning path. And within the same concurrent.locks package, there is also a lock interface, and this includes methods in order to lock and unlock resources. Implementations of the lock interface include ReentrantLocks as well as StampedLocks. ReentrantLocks allow threads to acquire the same locks multiple times and StampedLocks allow one thread to acquire a lock and other threads to release then hold on it. So while this covers the use of locks in Java, in the next video we will take a look at another object which regulates the number of concurrent threads which can access a shared resource, specifically semaphores.</p></div>
</div>
<div>
<h2 id="a15de653-e022-4808-ad0b-f41fb8d64ad0">
Semaphores
</h2>
<div>[Video description begins] <em>Topic title: Semaphores. Your host for this session is Kishan Iyer.</em> [Video description ends]
<p>In the previous video, we covered the use of locks in order to ensure exclusive access for a thread to a particular shared resource. While in many cases, you do require exclusive access to prevent say the race condition. In other instances, it&#39;s not the race condition you&#39;re trying to tackle but the overall load on the system. In this case, there is another concurrency mechanism which can be employed, and this is known as a semaphore. So what exactly is a semaphore? Well, this is something which allows us to restrict the number of tasks which use a particular resource. Now this resource can be anything.</p>
<p>It could be a shared queue for example, which is used by many concurrent threads. It could also be a database or a server, which hosts video files, which may be accessed by several threads and processes. To make sure that the burden on the shared resources is not too high, a semaphore can effectively serve as a counter, which keeps track of all of the active tasks on that resource. Each time a task wishes to access that resource, it can increment the counter, and then decrement it once it&#39;s done with it. The question then is what exactly happens when the limit for a particular semaphore has been hit, and then another thread makes a request for that semaphore? Well, this becomes similar to a thread which requests for a lock, which is held by another thread.</p>
<p>That is, it simply needs to wait until one of the existing threads releases its hold on the semaphore. So now let&#39;s try to visualize how this works. So let&#39;s just say we have a shared resource such as a database, and we want to make sure that only three concurrent threads can access it at any given time. So we associate the number 3 with a semaphore and we quote our application in such a way where each thread needs to first acquire the semaphore before it accesses the database. At this point, there are no threads which need the database, so the counter for the semaphore is 3. But the moment a particular task acquires the semaphore. Well, the value of that counter drops to 2. So this is in some ways similar to acquiring a lock. However, the difference with semaphores is that access to the shared resource is not exclusive.</p>
<p>In this particular case, the semaphore has 2 permits to offer, which means that 2 more threads and acquire a hold on it. So let&#39;s just say while the first task is still going on, a second task acquires the same semaphore, which means that it&#39;s counter drops by 1, and now we have 2 concurrent threads which operate on the same resource. And now let&#39;s just say a third task has also acquired the semaphore. So while we have three concurrent tasks operating on the same database, what happens if 4 tasks wishes to access the same resource? Well, now that the limit for the semaphore has been hit, this task will simply have to wait. So with this implementation, we can ensure that the load on the database never exceeds a specified value. And this in turn can allow us a certain degree of concurrency while also avoiding the worst case scenario, where an overloaded database simply becomes unresponsive.</p>
<p>So what if one of the tasks has finished its own use of the database? Well, this is when it can release its code on the semaphore. And by doing so, it increments the associated counter. This means that one of the waiting threads can acquire that semaphore, which is precisely what happens with task number 4. So this is the fundamental principle of a semaphore. In some variations of this, you could in fact set different tasks to acquire different numbers of permits when they access a resource. For example, a task which requires access to multiple tables within the database and needs to run several queries may be programmed to require more than a single permit in order to access a resource through a semaphore. Whereas more lightweight tasks may only require one permit as we have seen in this example.</p></div>
</div>
<div>
<h2 id="8afdb5d9-a9b4-45df-b00c-820ed878608b">
Deadlocks
</h2>
<div>[Video description begins] <em>Topic title: Deadlocks. Your host for this session is Kishan Iyer.</em> [Video description ends]
<p>When it comes to concurrent programming, we have already covered the fact that one of the pitfalls is the occurrence of a race condition. We then saw how this can be addressed by making use of locks to grant exclusive access to a particular thread for a particular resource. So while this does solve one crucial problem, it can create another problem of its own, specifically a deadlock. We&#39;ll now take a closer look at how exactly deadlocks can occur in a multi-threaded or multiprocessing program, and we&#39;ll also explore some of the approaches when it comes to avoiding them. So what exactly is a deadlock?</p>
<p>Well, in the context of concurrent programming, this is a particular state which we can get to when each member of a particular group is waiting for another member to take some action before it can proceed on its own. The particular definition which you see on the screen has been sourced from Wikipedia and this can be applied to the different threads in a multi-threaded application. So we have different threads which are waiting on each other before they can proceed, and since each and every one of them is waiting on another thread, they&#39;re all effectively stuck, which means that they are deadlocked. So what exactly can cause a deadlock? In fact, there are four necessary conditions for a deadlock to occur, which means that in the absence of even one of these, a deadlock cannot occur. However, the first of these is the condition of mutual exclusion, which is precisely what locks are meant for.</p>
<p>The next condition is hold and wait. This is where a lock is held by a particular thread, and at the same time that thread puts in a request for another lock, which is held by a different thread. In such a case, the thread holds one lock while waiting for another. So when we have mutual exclusion as well as a hold and wait, if we add to that a particular set of circumstances which leads to a circular wait, then a deadlock could occur. What exactly is a circular wait? Well, this is something we will visualize in just a moment. So these three are necessary but not sufficient conditions for a deadlock and we need one more, and this is no preemption. Let&#39;s now take a closer look at each of these conditions. In the case of mutual exclusion, this means that only one process or thread can hold a specific resource at any given time, which is precisely what locks help us accomplish.</p>
<p>And then there is the hold and wait where a process happens to hold at least one resource and without releasing its own hold on those resources, it tries to acquire resources which are held by other threads. This can lead to a circular wait where we have multiple threads which are waiting for others to release their resources before they themselves can proceed. And all of these things can lead to a deadlock if there is no preemption which is possible in the program. This is precisely what happens when each thread or process can only give up its own lock voluntarily, and no other thread or process can force it to give it up. So this covers the theoretical occurrence of a deadlock, but it is better for us to visualize how this can occur to get a better understanding of this concept.</p>
<p>So let us assume that we have three different individuals, Alice, Bob, and Charles, and they plan to start work concurrently on their own tasks. And in order to carry out their tasks, they need to access three resources which are represented by these blocks A, B, and C. So in order for Alice to perform her own work, she acquires resource A, and let&#39;s just say Bob and Charles acquire resources B and C respectively. So all three of them have begun their work and all of this is happening concurrently. Furthermore, let&#39;s assume we have set things up in such a way that each of them has exclusive access to these resources. So this fulfils one of the four conditions for a deadlock, that is mutual exclusion. <br /><br />But then, of course, there are three more conditions which need to be fulfilled. Now let us assume that Alice, while she still has access to resource A, puts in a request for resource B, which is currently held by Bob. Well, the second condition for a deadlock has now been fulfilled, specifically hold and wait. Furthermore, let&#39;s assume that Bob, while still hanging on to resource B, puts in a request for resource C in order to move ahead. But that, of course, is being held by Charles and we once again enter into a state of hold and wait. In fact, there is more.</p>
<p>Our implementation ensures that each person can only voluntarily give up the resource which they currently hold, which means that nobody has the right to preempt access to a resource. So this fulfills the condition of no preemption, which again is one of the four conditions for a deadlock. So at this point, three of the four conditions for a deadlock are in place, but we can still get out of it. Assume, for example, that Charles eventually finishes his work on resource C and then gives it up to B, who in turn can finish his work and give up his hold on resource B and then unblock Alice.<br /><br />However, there is also a possibility that at this point, Charles in turn puts in a request for resource A and then decides to wait until Alice lets go of it. And this brings us the fourth and final condition for a deadlock, specifically a circular wait. So at this point in time, all the four necessary and sufficient conditions for a deadlock apply, which means that none of these threads can proceed, and this means that they are deadlocked.</p>
<p>In many ways, this is not unlike situations you see with traffic jams where no vehicles can really move ahead until someone is able to or decides to take a step back. So the question now is, what exactly can we do to avoid deadlocks in the first place? Well, a simple answer to this is to prevent even one of the four necessary conditions from taking place at all. This is easier said than done, however, but an example is for a process or thread to hold a lock on just one resource at a time. <br /><br />This eliminates the hold and wait condition and also ensures that concurrent threads never enter into a state of a deadlock. Another potential solution is to enable resource preemption. This is where you effectively have a priority set for different tasks so that tasks at a higher priority level have the ability to force tasks with a lower priority to release their resources. As long as each task has a different priority value, a deadlock will never occur in this case.</p>
<p>You&#39;ll notice, however, that each of these potential solutions does have a drawback. And in fact, any option to avoid deadlocks serves as a trade-off. In many cases, developers may choose not to prevent deadlocks in the first place. But simply to detect them quickly and have a plan in place to break out of it, even if this involves rolling back or even terminating some of the running threads. There are a variety of tools available in order to detect deadlocks, but those are beyond the scope of this learning path.</p></div>
</div>
<div>
<h2 id="90a5668f-09f5-43e7-91a0-ab9da1f2e628">
Concurrency Objects in Java
</h2>
<div>[Video description begins] <em>Topic title: Concurrency Objects in Java. Your host for this session is Kishan Iyer.</em> [Video description ends]
<p>From what we have covered in this course so far, what will be clear is that concurrent programming can get rather complicated. Avoiding race conditions alone can lead to a lot of extra programming work for developers, for example, where you require the use of locks when it comes to accessing shared data structures. However, since the solutions to this are pretty much standardized, Java has made things convenient for us, by including a number of different concurrency objects, which you can simply plug into our own applications. Let&#39;s now take a closer look at some of these. We have already covered in this course, that Java offers synchronization mechanisms such as synchronized functions and blocks. However, these are in fact limited in terms of features and the ability to scale.</p>
<p>In order to create and manage a large number of threads, and allow them to access shared data structures, we may be better off using the off the shelf concurrency objects available in the Java language. So what exactly are these concurrency objects? We have already discussed some of the lock objects. These include the lock and condition interfaces, as well as lock implementations, which include a lot of the standard locking features. These include the ability to acquire and release locks, acquire locks for read only purposes, as well as locks in order to perform an update, and many, many more. Furthermore, Java includes a number of different concurrent collections. These are thread-safe data structures, which prevent the occurrence of race condition when it comes to accessing and updating their contents. Beyond that, Java also offers volatile and atomic variables.</p>
<p>Again, these are to ensure thread-safe access but to primitive data types. And then we have the Java executors. These are meant to manage a pool of threads. We can simply initialize tasks and submit them to an executor, and it will take care of, not only its execution, but will also ensure a certain degree of concurrency. Let&#39;s now explore each of these in detail to get a better idea of the features of these concurrency objects and their use cases. Starting with Locks and Semaphores. We have already covered these from a theoretical standpoint, and in order to implement them within our Java code, we can make use of the tools available in the java.util.concurrent package. This includes a number of different interfaces as well as lock implementations.</p>
<p>As discussed previously, this includes a reentrant lock, as well as a stamped lock, and within the same package, we also have a semaphore class. We initialize each semaphore instance with a certain number of permits, and individual tasks can acquire and release a specific number of permits on the semaphore. Furthermore, a lot of tasks may require some form of timing and scheduling for which there is a built-in TimeUnit enum which can be employed when submitting tasks to an executor service, for example. Let&#39;s move along then, to some of the concurrent collections which are on offer. For a map data structure, Java offers the ConcurrentHashMap. This is in fact a synchronized version of a Java hashmap, which is in fact similar in some respects to a hash table. However, unlike a hash table, which requires a lock on the entire data structure, a ConcurrentHashMap allows a greater degree of concurrency by implementing locks for only a portion of the map data structure.</p>
<p>When it comes to ArrayLists, while the regular ArrayList is not synchronized in Java, a CopyOnWriteArrayList is. In this implementation, write operations are performed on a copy of the underlying array. And while this is happening, other threads and processes can still read from the original version. There is also a PriorityBlockingQueue which is a thread-safe version of the priority queue. This is a data structure which is unbounded, that is, there is no limit to the number of elements which you can add to the queue. Then there is the ArrayBlockingQueue. And this is a data structure which does have an upper bound. This is in fact the preferred data structure for the producer-consumer problem. So all of these data structures are thread-safe, which means that we don&#39;t need to explicitly define any form of locking mechanism to access it. All of that has already been baked in by Java. Let&#39;s move along then to volatile variables.</p>
<p>So by default variables in Java are non-volatile, which means that different threads are not guaranteed access to its latest value, even if it has been updated by a different thread. However, this is not the case when it comes to volatile variables. When a variable has been declared as volatile in Java, a thread which reads its value will perform that read, not from its local CPU cache, but from main memory. In fact, write operations of those variables will also happen to main memory directly. This ensures that all of the threads are dealing with the same source for the variables value, that is the location in main memory, rather than local copies which are maintained in the CPU cache. These local values, of course, can get out of date. Furthermore, volatile variables guarantee a happens-before relationship for operations.</p>
<p>So for example, if thread A were to perform a write before thread B has performed a read, the fact that they operate on the same location and main memory ensures that the actions of thread A occur before the action of thread B. Having taken a look at some of the concurrency objects in the Java programming language, in the next video, we&#39;ll turn our attention to one more. Specifically, atomic variables, which allow us to perform atomic operations on primitive data types.</p></div>
</div>
<div>
<h2 id="fac1110c-2fd3-45cd-9a98-3c687a810206">
Atomic Variables
</h2>
<div>[Video description begins] <em>Topic title: Atomic Variables. Your host for this session is Kishan Iyer.</em> [Video description ends]
<p>Having explored some of the concurrency mechanisms and objects which are available in the Java language, we now turn our attention towards atomic variables which allow us to carry out atomic operations on primitive data types. And for this, consider the case where in a multi-threaded application, each individual thread may access primitive types in order to carry out read and write operations which need to be atomic in nature. <br /><br />That is, certain read and write operations need to be performed as a single unit and should not be interleaved with different threads. To understand this, let&#39;s now try to simulate a race condition once again. So we have two different threads. And each of these needs to access a shared integer x, which is initialized to a value of 100. You&#39;ll observe that the code in each of these means that they first perform an update to the value and then access that updated value.</p>
<p>Let&#39;s now explore the ideal scenario. Where thread number 1 is allowed to proceed, so it updates the value of x which means that it has been incremented to 101. And then, when it invokes return x, thread number 1 is able to view its own update. And this of course, is only possible because this thread is able to execute both of its lines as though they were one unit. That is they were executed as a block. So now that thread 1 is done, thread 2 is allowed to proceed and it starts off by decrementing x whose value has now been updated to 100 again, and when return access invoked, again, thread 2 is able to see its own update. And you can imagine that this is in fact, a common operation when a multi threaded application is running. Where a thread updates a primitive type and then reads its own update.</p>
<p>We will see in a moment that this is only possible if the update and the following read are treated as a single unit that is they&#39;re atomic. So what is the alternative? Well, let&#39;s assume that the actions of each of the threads happen to interleave. So these are concurrent threads and if thread 1 happens to go first, it will increment the value of x to 101. If execution then switches over to thread number 2, this performs a decrement operation, so x goes back to a value of 100. But now, when execution switches back to thread one and return x is invoked, the value of x, which has been read by thread 1 is different from what we saw previously. And in fact, this read operation does not reflect the update, which was performed by thread 1.</p>
<p>However, when it comes to thread 2, it is able to view its own update. And you will know by now that what we have just taken a look at is a classic race condition. And you might be thinking, well, this can very easily be avoided with locks. And in fact you are right, locks are a solution to this problem. However, locks do come with several issues of their own. First of all, they do involve a lot of overhead. For example, a thread which is waiting on a resource will need to be suspended, and then resumed once that resource becomes available. All of this added cost, does make sense if there are larger blocks of code involved.</p>
<p>For example, if you happen to be carrying out several updates, or there is a more complex sequence of read and write operations, but implementing locks in order to perform a simple update and read could drastically slow down your application, especially if this is something which happens very often. Furthermore, all of the writing and maintaining for this extra code involving the locking and unlocking adds to the overall code complexity of your application. Beyond all of this, you may find that locks are not ideal because they are by nature pessimistic. They assume the worst from the other threads that they will perform an update, which will lead to a bad outcome, which is why they grant exclusive access to a resource to a single thread.</p>
<p>While this may be the best approach in some cases, atomic variables work a little differently. These effectively serve as a wrappers around primitive data types. And rather than locking access to these resources, they adopt the compare and swap technique which by nature is more optimistic. This means that in order to carry out any operation, whether to read the value of an atomic variable, or to update it, threads don&#39;t end up blocking each other. However, what they do is to perform a check of the memory location before they carry out a write operation.</p>
<p>And then, a write is only performed if the current value happens to be different from the expected value. While the specific details of compare and swap are beyond the scope of this course. All in all, this particular method involves a lot less overhead than locking. So what exactly are the atomic variables which are available in the Java programming language? Well, these include atomic integers, atomic Boolean values, atomic integer arrays, and a few more. And in order to work with atomic variables, Java does provide a number of different atomic functions.</p>
<p>These include getAndAdd, where the value of a variable is read, and then another value is added to it. And this applies to the numeric atomic variables. And then there is an incrementAndGet, where the variable is first incremented and then its value is read. These atomic operations are far more lightweight than using locks to perform the same steps. And this is thanks to the compare and swap technique. The big picture view for the developers however, is that thanks to atomic variables, it is possible for us to achieve atomicity for operations with far fewer lines of code, especially when compared to using locks.</p></div>
</div>
<div>
<h2 id="96f23caf-e35e-4f84-a5c1-e98f97cd05a7">
Executors and Thread Pools
</h2>
<div>[Video description begins] <em>Topic title: Executors and Thread Pools. Your host for this session is KishanIyer.</em> [Video description ends]
<p>For a multi-threaded Java application, which is meant to operate at scale, thread management can become rather painstaking. However, this can be greatly simplified with the use of executors and thread pools, which we will now take a closer look at. So let&#39;s just say we have many different tasks which are created in our program, and we need these to run on threads. Many of these can be run concurrently, but a rather simplistic way to handle the execution of these tasks is to create and run a new thread for each individual task.</p>
<p>You can imagine that if the number of tasks happens to be very large, well this will also involve the creation of multiple threads, and then managing them. Furthermore, if you want to limit the degree of concurrency, let&#39;s just say in order to prevent the system. From spawning more threads than it can handle, well this will require additional work on your part.</p>
<p>So in other words, scaling up is not very easy, especially when you have to do it on your own. Furthermore, writing a lot of code in order to limit the number of threads and also to manage their executions can make your source code harder to read and to maintain. Since this is a rather common problem encountered by most developers working in a multi-threaded setting. Java does provide an off the shelf solution for this. Where you don&#39;t create separate threads for each task which needs to be executed and then manage those threads. But instead, you define your task and simply submit them over to a thread pool.</p>
<p>So what exactly is a thread pool? Well, assume that you have created four different threads which are grouped together into a single unit, and we call this unit, a thread pool. Each individual thread here can be treated as a separate worker thread, and by making sure that these are the only threads in your application. Well, you can effectively limit the number of concurrent threads to four. This means that all of the task executions will be handled by these four threads, no matter how many tasks are created. How exactly can this work? Well, let&#39;s take an example where there are seven different tasks and all of these can be executed concurrently. However, we don&#39;t spawn seven different threads for this but instead submit all of them over to the thread pool. When that happens, all of the seven tasks are placed in a queue. And then each of the workers in the pool can pick up one task from the head of the queue and then begin working on them.</p>
<p>So at this point four of the tasks are being processed concurrently, while there are three others which are waiting in the queue. After some time, let&#39;s just say one of the tasks has been completed by one of the workers, which means that it is now free pick up the next task from the queue. So it goes along and picks up task number five, and begins processing that. So we once again back to four concurrently executing tasks. Some more time has passed and another task has been completed by one of the workers. So it&#39;s now free to pick off task number six from the head of the queue. And once it does that, let&#39;s just say two more workers have also completed their respective tasks. At this point, there is just a single task waiting to be executed. So one of the workers picks that up.</p>
<p>So we are now up to three concurrently executing tasks, while one of the workers remains free until a new task is submitted to the queue. However, if that does not happen soon enough, there will be a point where all seven tasks have been completed, and all the workers are now free once again. The management of thread pools in the Java programming language is managed by the executor service. So let&#39;s now take a closer look at executors by creating a thread pool of a fixed size. They effectively limit the number of threads which are created by the program.</p>
<p>This prevents the number of spawned threats from going out of control, and we don&#39;t really need to take care of the management. All we need to do is to initialize an executor with a set number of threads. Furthermore, the same threats can be reused for the different tasks. So we don&#39;t need to constantly spawn new threads and terminate them, which in turn does save on overhead further more the task can be submitted directly to the executor service, rather than to individual worker threads. It&#39;s up to the executor service to make sure that the task is assigned to a free worker. Once it becomes available, again, this is something we don&#39;t need to implement on our own and is handled directly by Java. Furthermore, Java also includes a variant of the executor service called the scheduled executor service, which allows task executions to be scheduled at a later point in the future.</p>
<p>Beyond this, it&#39;s also possible for recurring tasks to be submitted to the scheduled executor service with a specific interval, which conveys the frequency with which that task needs to be executed, and also specific delay. All in all, the executor service manages the execution of tasks. All we need to do is to define the task itself and submit it over to the executor service. And it will take care of the execution whether it is on a one off basis or even if it is one which needs to be scheduled or executed repeatedly. And this takes us to a question of implementation. Specifically, what exactly is the nature of a task which can be submitted to an executor? Well, one of these is a runnable task. We have previously discussed the fact that a class which extends the runnable interface can have instances run in a separate thread. And this also means that they can be submitted over to a Java executor.</p>
<p>Now keep in mind that a runnable task is one which does not return a value. However, similar to runnable task Java also includes callable tasks. Like runnables, these can also be executed in individual threads but what distinguishes them is the fact that these return a value. The fact that both runnable as well as callable tasks can run on separate threads means that it is possible for them to be submitted to an executor service, which of course will assign these over to a worker thread. Furthermore, when we do have a runnable or callable task, since both of these are interfaces in Java, using them will require some functions to be implemented. In the case of a runnable task, this is the run() function, whereas for callable tasks, this is the call() function.</p></div>
</div>
<div>
<h2 id="e241a71a-8372-4fd8-b854-4e615c39cfd0">
Future Objects and Fork/Join
</h2>
<div>[Video description begins] <em>Topic title: Future Objects and Fork/Join. Your host for this session is Kishan Iyer.</em> [Video description ends]
<p>So far in this course, we have covered a lot of the mechanisms as well as the objects which are available in the Java programming language in order to work with concurrent threads. We now continue with that and take a look at the futures object as well as the fork/join framework. Starting with the futures object, so we have already covered the fact that the type of tasks which can be submitted to an executor in Java include a runnable task, and this is one which does not return a value, and then there is a callable task, one which does. Specifically in the case of a callable task, there may be another thread, which is waiting for this task to return a value before it can proceed with its own execution. In other words, this thread may need to synchronize its own execution with a specific callable task which is running on a thread.</p>
<p>The question is, is it possible to do that? Well, the answer is yes. And this is where a future object comes into the picture. A future object in Java represents an asynchronous task execution and it allows us to keep an eye on this task, and in the case of a callable task, to also access its return value once it is available. A future object can also be used to keep track of a runnable task. So this is often associated with a callable, which returns a value. So how exactly can this work? Well, we have already covered the fact that an executor contains a pool of workers. So let&#39;s just say that we have a callable task which is submitted to this. And given that this is an asynchronous task, well, it will simply be executed by one of the workers in the pool, while the thread which submits this task can go ahead with its own execution.</p>
<p>However, there may be a requirement for some thread to keep an eye on this execution of the callable task and access its return value once it becomes available. And precisely for this reason, when a task is submitted to an executor, what it returns is a future object. So the question is, what exactly does the future have in store? What exactly is inside a future object? Let&#39;s now take a closer look. First of all, the future object allows us to access the future result of the execution. Not that it can tell what the result will be, but it allows us to access it once it becomes available. <br /><br />Beyond that, this object includes a function called isDone() and we can invoke this in order to check whether the task is complete. This can be used to monitor, not just callable, but also runnable tasks. And, if it is a callable task, which the future object represents, and if a thread needs to synchronize its own execution with the availability of that result, well, it can simply invoke the future object&#39;s get() method. When this is done, the calling thread will block if the result is not yet available, and it will return and proceed with its own execution once it is.</p>
<p>Furthermore, if the execution is taking way too long, well, it is possible to invoke the cancel function of the future object in order to terminate its execution. So, these are some of the common operations which can be performed with a future object. We move along then from the management of individual task executions, to the management of really large tasks, which could be broken down into a number of smaller and manageable sub-tasks. And we can do this by using the fork/join framework in Java. This is effectively a divide and conquer approach to parallel programming. If you have a background in big data, and are familiar with the MapReduce programming model, you can almost think of the fork/join framework as similar to MapReduce, where the fork is analogous to map and join is analogues to the reduce operation.</p>
<p>What exactly does this mean in the context of concurrent programming? Well, consider that we have a really large task which could potentially be broken down into a number of smaller sub-tasks, which can be executed in an asynchronous manner. The fork/join framework allows us to do exactly that by defining just a few operations. Similar to runnable, and callable tasks, which we have already covered, Java also includes a ForkJoinTask, and this is one which includes the logic to break up a large task into sub-tasks if it exceeds a certain threshold. Or, if the submitted task is small enough, it will simply go ahead and execute it like a normal task. So the breaking up of a large task into sub-tasks is the divide phase of the divide and conquer approach, and then executing the sub-task itself is the conquer phase. Furthermore, a ForkJoinTask is submitted to a special variation of the executor service, and this is known as a ForkJoinPool.</p>
<p>This is able to handle the logic of the ForkJoinTask in order to perform a break up into sub-tasks if needed. And to perform the execution when the sub-task is small enough. The spawning of the worker threads, the submission of the sub-tasks to them is all handled by the framework. Let&#39;s now take a look at what exactly a ForkJoinTask looks like in terms of code. So here is some pseudocode for the ForkJoinTask and let us walk through the different components. Before we do that though, keep in mind that a ForkJoinTask is something which is submitted to a ForkJoinPool. Beyond that, ForkJoinTask is in fact an interface, and there are a couple of different implementations of this. These include a recursive task, which is one which returns a result, and then there is a recursive action, one which does not. These are in fact similar to the runnable and callable tasks, except that ForkJoinTasks can be recursively broken down into smaller sub-tasks.</p>
<p>And how exactly can this happen? Well, this is where we define the compute() function of a ForkJoinTask, similar to the run() of a runnable, or call() of a callable task. This is one which needs to be defined for a ForkJoinTask, and let&#39;s take a closer look at what exactly we have defined here. First, we check whether the submitted task is small enough, given our own logic. And if it is, we simply go ahead and execute the task. If it needs to return a result, well, it will do precisely that. On the other hand, if the task is deemed too big to be executed directly, well, we effectively submit a number of sub-tasks over to a ForkJoinPool. And in this particular example, we have a separate function called divide into sub-tasks to handle this.</p>
<p>Keep in mind that this is a recursive call. Each time when the submitted task is deemed too large, it will recursively divide into smaller sub-task, until each sub-task is small enough to be executed directly. Furthermore, if the overall ForkJoinTask needs to return a result, well, there needs to be some logic defined in order to combine the results from each of the sub-tasks into an aggregate result. What this means for developers though, is that we only need to define the breakdown and combine logic, if required. And the executions themselves will be handled by the fork/join framework. And this is one more example of how the Java programming language makes concurrent programming just a little bit easier.</p></div>
</div>
<div>
<h2 id="98619706-a53d-4f55-9a5d-61fce5a67e18">
Course Summary
</h2>
<div>[Video description begins] <em>Topic title: Course Summary</em> [Video description ends]
<p>Having completed this theoretical course on concurrent programming in Java, we can now do a quick recap of the topics which were covered. We started off by exploring the concept of multi threading and multi processing, the benefits of these approaches, as well as the potential pitfalls such as the inconsistencies produced by threads which are not synchronized. We then looked into various synchronization options in the Java programming language from synchronized functions and blocks, to the use of locks, including issues such as deadlocks, which may occur when using them.</p>
<p>We then covered some of the advanced topics related to multi threading in Java, from executors which run tasks using a pool of threads to the built in data structures, which make common operations thread safe. Having completed this course you will have a clear picture of what concurrent programming in Java involves as well as the objects and frameworks which are available to implement multi threading. You are now ready to move on to a hands-on course covering the spawning and execution of concurrent threads in Java.</p></div>
</div>
</body>
</html>
